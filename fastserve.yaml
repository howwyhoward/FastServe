# FastServe Configuration File
# Customize settings for your Mac M2 deployment

# Model Configuration
model:
  name: "microsoft/DialoGPT-small"  # Start with small model for testing
  max_sequence_length: 1024
  max_new_tokens: 256

# Device Configuration (auto-detected, but can override)
device:
  type: "auto"  # Options: auto, mps, cpu, cuda
  dtype: "auto"  # auto, float16, float32

# Scheduler Configuration
scheduler:
  num_priority_queues: 4
  queue_capacity: 100
  preemption_enabled: true
  skip_threshold: 0.7
  promotion_threshold: 5
  demotion_threshold: 20

# Memory Management
memory:
  gpu_memory_threshold: 0.8
  max_cache_size_mb: 1024  # Adjust based on your Mac's memory
  enable_memory_offloading: true

# Server Configuration
server:
  host: "0.0.0.0"
  port: 8000
  max_concurrent_requests: 10
  log_level: "INFO"

# Performance Tuning
performance:
  batch_size: 1
  prefill_batch_size: 4
  decode_batch_size: 8

# Mac M2 Specific Optimizations
mac_m2:
  # Recommended settings for different memory configurations
  
  # For 8GB Mac M2
  memory_8gb:
    max_cache_size_mb: 512
    max_concurrent_requests: 4
    
  # For 16GB Mac M2  
  memory_16gb:
    max_cache_size_mb: 1024
    max_concurrent_requests: 8
    
  # For 24GB Mac M2
  memory_24gb:
    max_cache_size_mb: 2048
    max_concurrent_requests: 12
